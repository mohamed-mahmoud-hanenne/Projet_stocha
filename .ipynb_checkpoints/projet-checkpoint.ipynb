{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c043a5e4-8735-4bbb-9dcf-5d1fe5f625a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Charger les données depuis le fichier CSV\n",
    "df_prices = pd.read_csv(\"stock_prices.csv\")\n",
    "\n",
    "# Définition des paramètres globaux\n",
    "alpha = 0.1  # Taux d'apprentissage\n",
    "gamma = 0.9  # Réduction des récompenses futures\n",
    "epsilon = 0.1  # Exploration (probabilité d'essayer une action aléatoire)\n",
    "\n",
    "# Fonction Q-Learning\n",
    "def q_learning_prices(df):\n",
    "    actions = [\"buy\", \"sell\", \"hold\"]\n",
    "    states = len(df)\n",
    "    Q = np.zeros((states, len(actions)))\n",
    "\n",
    "    def reward(action, price_today, price_tomorrow):\n",
    "        if action == \"buy\":\n",
    "            return price_tomorrow - price_today\n",
    "        elif action == \"sell\":\n",
    "            return price_today - price_tomorrow\n",
    "        return 0\n",
    "\n",
    "    for episode in range(10):\n",
    "        state = 0\n",
    "        while state < states - 1:\n",
    "            if np.random.rand() < epsilon:  # Exploration\n",
    "                action_index = np.random.randint(len(actions))\n",
    "            else:  # Exploitation\n",
    "                action_index = np.argmax(Q[state])\n",
    "\n",
    "            action = actions[action_index]\n",
    "            next_state = state + 1\n",
    "            r = reward(action, df[\"Price\"].iloc[state], df[\"Price\"].iloc[next_state])\n",
    "\n",
    "            # Mise à jour de la table Q\n",
    "            Q[state, action_index] += alpha * (\n",
    "                r + gamma * np.max(Q[next_state]) - Q[state, action_index]\n",
    "            )\n",
    "            state = next_state\n",
    "\n",
    "    return Q\n",
    "\n",
    "# Exécution de l'algorithme\n",
    "Q_table_prices = q_learning_prices(df_prices)\n",
    "print(\"Table Q pour les prix des actions :\\n\", Q_table_prices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977d3494-1236-4e54-9c83-c083bd16344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. SARSA (Base : Prix des actions)\n",
    "\n",
    "def sarsa_prices(df):\n",
    "    actions = [\"buy\", \"sell\", \"hold\"]\n",
    "    states = len(df)\n",
    "    Q = np.zeros((states, len(actions)))\n",
    "\n",
    "    def reward(action, price_today, price_tomorrow):\n",
    "        if action == \"buy\":\n",
    "            return price_tomorrow - price_today\n",
    "        elif action == \"sell\":\n",
    "            return price_today - price_tomorrow\n",
    "        return 0\n",
    "\n",
    "    for episode in range(10):\n",
    "        state = 0\n",
    "        if np.random.rand() < epsilon:\n",
    "            action_index = np.random.randint(len(actions))\n",
    "        else:\n",
    "            action_index = np.argmax(Q[state])\n",
    "\n",
    "        while state < states - 1:\n",
    "            action = actions[action_index]\n",
    "            next_state = state + 1\n",
    "            r = reward(action, df[\"Price\"][state], df[\"Price\"][next_state])\n",
    "\n",
    "            if np.random.rand() < epsilon:\n",
    "                next_action_index = np.random.randint(len(actions))\n",
    "            else:\n",
    "                next_action_index = np.argmax(Q[next_state])\n",
    "\n",
    "            Q[state, action_index] += alpha * (\n",
    "                r + gamma * Q[next_state, next_action_index] - Q[state, action_index]\n",
    "            )\n",
    "\n",
    "            state = next_state\n",
    "            action_index = next_action_index\n",
    "\n",
    "    return Q\n",
    "\n",
    "# Exécution\n",
    "Q_table_sarsa = sarsa_prices(df_prices)\n",
    "print(\"Table Q pour SARSA :\\n\", Q_table_sarsa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862d46f1-659b-436e-84c9-1d4b80094556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Deep Q-Learning (Base : Gestion de portefeuille)\n",
    "\n",
    "def deep_q_learning_portfolio(df):\n",
    "    # Supprimer la colonne 'Date' si elle existe\n",
    "    df = df.drop(columns=['Date'], errors='ignore')\n",
    "    \n",
    "    # Convertir les colonnes en numérique\n",
    "    df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "    if df.shape[0] == 0:\n",
    "        raise ValueError(\"Le DataFrame est vide après nettoyage. Vérifiez vos données.\")\n",
    "\n",
    "    num_states = df.shape[1]  # Nombre d'entrées (Stock_A, Stock_B, Stock_C)\n",
    "    num_actions = num_states  # Nombre d'actions possibles : investir dans Stock_A, Stock_B, Stock_C\n",
    "\n",
    "    # ✅ Correction : Définition correcte du modèle avec `Input(shape=...)`\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(num_states,)),  # Correctement défini\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(num_actions, activation=\"linear\")\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "    gamma = 0.9\n",
    "    epsilon = 0.1\n",
    "    epochs = 10\n",
    "    memory = []\n",
    "    max_memory = 1000\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        state = df.iloc[0].values.astype(float)\n",
    "        for t in range(1, len(df)):\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.randint(0, num_actions)\n",
    "            else:\n",
    "                q_values = model.predict(state.reshape(1, -1), verbose=0)\n",
    "                action = np.argmax(q_values)\n",
    "\n",
    "            next_state = df.iloc[t].values.astype(float)\n",
    "            reward = next_state[action]\n",
    "\n",
    "            memory.append((state, action, reward, next_state))\n",
    "            if len(memory) > max_memory:\n",
    "                memory.pop(0)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if len(memory) > 32:\n",
    "                batch = random.sample(memory, 32)\n",
    "                states, actions, rewards, next_states = zip(*batch)\n",
    "\n",
    "                states = np.array(states)\n",
    "                next_states = np.array(next_states)\n",
    "                q_values = model.predict(states, verbose=0)\n",
    "                q_next = model.predict(next_states, verbose=0)\n",
    "\n",
    "                for i in range(32):\n",
    "                    q_values[i, actions[i]] = rewards[i] + gamma * np.max(q_next[i])\n",
    "\n",
    "                model.fit(states, q_values, verbose=0)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Exécution\n",
    "model_portfolio = deep_q_learning_portfolio(df_portfolio)\n",
    "print(\"Modèle DQN entraîné pour la gestion de portefeuille.\",model_portfolio)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913bb3c4-e2e1-4b0d-a93a-f55cac18605e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Double Q-Learning (Base : Gestion de portefeuille)\n",
    "\n",
    "def double_q_learning_portfolio(df):\n",
    "    actions = [\"invest_A\", \"invest_B\", \"invest_C\"]\n",
    "    states = len(df)\n",
    "    Q1 = np.zeros((states, len(actions)))\n",
    "    Q2 = np.zeros((states, len(actions)))\n",
    "\n",
    "    for episode in range(10):\n",
    "        state = 0\n",
    "        while state < states - 1:\n",
    "            if np.random.rand() < epsilon:\n",
    "                action_index = np.random.randint(len(actions))\n",
    "            else:\n",
    "                action_index = np.argmax(Q1[state] + Q2[state])\n",
    "\n",
    "            next_state = state + 1\n",
    "            reward = df.iloc[next_state, action_index + 1]\n",
    "\n",
    "            if np.random.rand() < 0.5:\n",
    "                Q1[state, action_index] += alpha * (\n",
    "                    reward + gamma * np.max(Q2[next_state]) - Q1[state, action_index]\n",
    "                )\n",
    "            else:\n",
    "                Q2[state, action_index] += alpha * (\n",
    "                    reward + gamma * np.max(Q1[next_state]) - Q2[state, action_index]\n",
    "                )\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "    return Q1, Q2\n",
    "\n",
    "# Exécution\n",
    "Q1, Q2 = double_q_learning_portfolio(df_portfolio)\n",
    "print(\"Tables Q1 et Q2 pour Double Q-Learning :\\n\", Q1, Q2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a55372-9637-4265-a46c-53cfb50e1841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.max(Q_table_prices, axis=1), label=\"Q-learning (Prix d'actions)\")\n",
    "plt.plot(np.max(Q_table_sarsa, axis=1), label=\"SARSA (Prix d'actions)\")\n",
    "plt.title(\"Comparaison des algorithmes pour les prix d'actions\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
